---
format: gfm
engine: julia
---

# ProDAG

Julia implementation of ProDAG from the paper ["ProDAG: Projection-induced variational inference for directed acyclic graphs"](https://arxiv.org/abs/2405.15167).

## Installation

To install `ProDAG` from GitHub, run the following code:

```{julia}
#| eval: false
using Pkg
Pkg.add(url = "https://github.com/ryan-thompson/ProDAG.jl")
```

## Usage

The `fit_linear()` function learns the posterior over linear DAGs given data `x`. The `sample()`
function draws DAGs from the learned posterior.

```{julia}
using ProDAG, CUDA, Random

CUDA.seed!(1)
Random.seed!(1)

# Generate some data
n = 100
p = 5
x = randn(n, p)

# Fit a posterior with a N(0,1) prior on each weight
fit = fit_linear(x, prior_μ = 0, prior_σ = 1, verbose = false)

# Draw a sample of DAGs from the posterior
w = sample(fit, n_sample = 3)
```


To learn a posterior over nonlinear DAGs in the form of acyclic multilayer perceptrons (MLPs), use the `fit_mlp()` function.

```{julia}
CUDA.seed!(1)
Random.seed!(1)

# Fit a posterior with a single hidden-layer of 10 neurons and a N(0,1) prior on each weight
fit = fit_mlp(x, hidden_layers = [10], bias = false, prior_μ = 0, prior_σ = 1, verbose = false)

# Draw a sample of DAGs from the posterior
w, model = sample(fit, n_sample = 3)

# The vector called "model" contains acyclic MLPs drawn from the posterior
x_new = randn(Float32, 5, p)
x̂ = model[1](x_new')'

```